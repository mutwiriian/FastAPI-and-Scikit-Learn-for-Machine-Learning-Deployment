{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates the data preparation and modelling steps taken in the development of machine learning models to predict temperature for ...\n",
    "\n",
    "The reason for building machine learning models is to serve them in production and make inferences from them. However, for most data scientists, especially those at the intermediate level, e.g yours truly, model development stops after tuning and selection of the best parameters. This project is a demonstration of how a Machine Learning Model can be served as an API endpoint which can be integrated into larger systems like recommender systems and other microservices."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we do the basic imports and configurations. Other import are done ont he fly for ease of following along on what each code chunk does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read in the data and get a snapshot of its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number_of_elements</th>\n",
       "      <th>mean_atomic_mass</th>\n",
       "      <th>wtd_mean_atomic_mass</th>\n",
       "      <th>gmean_atomic_mass</th>\n",
       "      <th>wtd_gmean_atomic_mass</th>\n",
       "      <th>entropy_atomic_mass</th>\n",
       "      <th>wtd_entropy_atomic_mass</th>\n",
       "      <th>range_atomic_mass</th>\n",
       "      <th>wtd_range_atomic_mass</th>\n",
       "      <th>std_atomic_mass</th>\n",
       "      <th>...</th>\n",
       "      <th>wtd_mean_Valence</th>\n",
       "      <th>gmean_Valence</th>\n",
       "      <th>wtd_gmean_Valence</th>\n",
       "      <th>entropy_Valence</th>\n",
       "      <th>wtd_entropy_Valence</th>\n",
       "      <th>range_Valence</th>\n",
       "      <th>wtd_range_Valence</th>\n",
       "      <th>std_Valence</th>\n",
       "      <th>wtd_std_Valence</th>\n",
       "      <th>critical_temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>88.944468</td>\n",
       "      <td>57.862692</td>\n",
       "      <td>66.361592</td>\n",
       "      <td>36.116612</td>\n",
       "      <td>1.181795</td>\n",
       "      <td>1.062396</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>31.794921</td>\n",
       "      <td>51.968828</td>\n",
       "      <td>...</td>\n",
       "      <td>2.257143</td>\n",
       "      <td>2.213364</td>\n",
       "      <td>2.219783</td>\n",
       "      <td>1.368922</td>\n",
       "      <td>1.066221</td>\n",
       "      <td>1</td>\n",
       "      <td>1.085714</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>0.437059</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>92.729214</td>\n",
       "      <td>58.518416</td>\n",
       "      <td>73.132787</td>\n",
       "      <td>36.396602</td>\n",
       "      <td>1.449309</td>\n",
       "      <td>1.057755</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>36.161939</td>\n",
       "      <td>47.094633</td>\n",
       "      <td>...</td>\n",
       "      <td>2.257143</td>\n",
       "      <td>1.888175</td>\n",
       "      <td>2.210679</td>\n",
       "      <td>1.557113</td>\n",
       "      <td>1.047221</td>\n",
       "      <td>2</td>\n",
       "      <td>1.128571</td>\n",
       "      <td>0.632456</td>\n",
       "      <td>0.468606</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>88.944468</td>\n",
       "      <td>57.885242</td>\n",
       "      <td>66.361592</td>\n",
       "      <td>36.122509</td>\n",
       "      <td>1.181795</td>\n",
       "      <td>0.975980</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>35.741099</td>\n",
       "      <td>51.968828</td>\n",
       "      <td>...</td>\n",
       "      <td>2.271429</td>\n",
       "      <td>2.213364</td>\n",
       "      <td>2.232679</td>\n",
       "      <td>1.368922</td>\n",
       "      <td>1.029175</td>\n",
       "      <td>1</td>\n",
       "      <td>1.114286</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>0.444697</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>88.944468</td>\n",
       "      <td>57.873967</td>\n",
       "      <td>66.361592</td>\n",
       "      <td>36.119560</td>\n",
       "      <td>1.181795</td>\n",
       "      <td>1.022291</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>33.768010</td>\n",
       "      <td>51.968828</td>\n",
       "      <td>...</td>\n",
       "      <td>2.264286</td>\n",
       "      <td>2.213364</td>\n",
       "      <td>2.226222</td>\n",
       "      <td>1.368922</td>\n",
       "      <td>1.048834</td>\n",
       "      <td>1</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>0.440952</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>88.944468</td>\n",
       "      <td>57.840143</td>\n",
       "      <td>66.361592</td>\n",
       "      <td>36.110716</td>\n",
       "      <td>1.181795</td>\n",
       "      <td>1.129224</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>27.848743</td>\n",
       "      <td>51.968828</td>\n",
       "      <td>...</td>\n",
       "      <td>2.242857</td>\n",
       "      <td>2.213364</td>\n",
       "      <td>2.206963</td>\n",
       "      <td>1.368922</td>\n",
       "      <td>1.096052</td>\n",
       "      <td>1</td>\n",
       "      <td>1.057143</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>0.428809</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 82 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   number_of_elements  mean_atomic_mass  wtd_mean_atomic_mass   \n",
       "0                   4         88.944468             57.862692  \\\n",
       "1                   5         92.729214             58.518416   \n",
       "2                   4         88.944468             57.885242   \n",
       "3                   4         88.944468             57.873967   \n",
       "4                   4         88.944468             57.840143   \n",
       "\n",
       "   gmean_atomic_mass  wtd_gmean_atomic_mass  entropy_atomic_mass   \n",
       "0          66.361592              36.116612             1.181795  \\\n",
       "1          73.132787              36.396602             1.449309   \n",
       "2          66.361592              36.122509             1.181795   \n",
       "3          66.361592              36.119560             1.181795   \n",
       "4          66.361592              36.110716             1.181795   \n",
       "\n",
       "   wtd_entropy_atomic_mass  range_atomic_mass  wtd_range_atomic_mass   \n",
       "0                 1.062396          122.90607              31.794921  \\\n",
       "1                 1.057755          122.90607              36.161939   \n",
       "2                 0.975980          122.90607              35.741099   \n",
       "3                 1.022291          122.90607              33.768010   \n",
       "4                 1.129224          122.90607              27.848743   \n",
       "\n",
       "   std_atomic_mass  ...  wtd_mean_Valence  gmean_Valence  wtd_gmean_Valence   \n",
       "0        51.968828  ...          2.257143       2.213364           2.219783  \\\n",
       "1        47.094633  ...          2.257143       1.888175           2.210679   \n",
       "2        51.968828  ...          2.271429       2.213364           2.232679   \n",
       "3        51.968828  ...          2.264286       2.213364           2.226222   \n",
       "4        51.968828  ...          2.242857       2.213364           2.206963   \n",
       "\n",
       "   entropy_Valence  wtd_entropy_Valence  range_Valence  wtd_range_Valence   \n",
       "0         1.368922             1.066221              1           1.085714  \\\n",
       "1         1.557113             1.047221              2           1.128571   \n",
       "2         1.368922             1.029175              1           1.114286   \n",
       "3         1.368922             1.048834              1           1.100000   \n",
       "4         1.368922             1.096052              1           1.057143   \n",
       "\n",
       "   std_Valence  wtd_std_Valence  critical_temp  \n",
       "0     0.433013         0.437059           29.0  \n",
       "1     0.632456         0.468606           26.0  \n",
       "2     0.433013         0.444697           19.0  \n",
       "3     0.433013         0.440952           22.0  \n",
       "4     0.433013         0.428809           23.0  \n",
       "\n",
       "[5 rows x 82 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_data=pd.read_csv('data/train.csv')\n",
    "temp_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate descriptive statistics of the features in the data ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>number_of_elements</th>\n",
       "      <td>21263.0</td>\n",
       "      <td>4.115224</td>\n",
       "      <td>1.439295</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>9.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_atomic_mass</th>\n",
       "      <td>21263.0</td>\n",
       "      <td>87.557631</td>\n",
       "      <td>29.676497</td>\n",
       "      <td>6.941000</td>\n",
       "      <td>72.458076</td>\n",
       "      <td>84.922750</td>\n",
       "      <td>100.404410</td>\n",
       "      <td>208.9804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wtd_mean_atomic_mass</th>\n",
       "      <td>21263.0</td>\n",
       "      <td>72.988310</td>\n",
       "      <td>33.490406</td>\n",
       "      <td>6.423452</td>\n",
       "      <td>52.143839</td>\n",
       "      <td>60.696571</td>\n",
       "      <td>86.103540</td>\n",
       "      <td>208.9804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gmean_atomic_mass</th>\n",
       "      <td>21263.0</td>\n",
       "      <td>71.290627</td>\n",
       "      <td>31.030272</td>\n",
       "      <td>5.320573</td>\n",
       "      <td>58.041225</td>\n",
       "      <td>66.361592</td>\n",
       "      <td>78.116681</td>\n",
       "      <td>208.9804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wtd_gmean_atomic_mass</th>\n",
       "      <td>21263.0</td>\n",
       "      <td>58.539916</td>\n",
       "      <td>36.651067</td>\n",
       "      <td>1.960849</td>\n",
       "      <td>35.248990</td>\n",
       "      <td>39.918385</td>\n",
       "      <td>73.113234</td>\n",
       "      <td>208.9804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>range_Valence</th>\n",
       "      <td>21263.0</td>\n",
       "      <td>2.041010</td>\n",
       "      <td>1.242345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wtd_range_Valence</th>\n",
       "      <td>21263.0</td>\n",
       "      <td>1.483007</td>\n",
       "      <td>0.978176</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.921454</td>\n",
       "      <td>1.063077</td>\n",
       "      <td>1.918400</td>\n",
       "      <td>6.9922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_Valence</th>\n",
       "      <td>21263.0</td>\n",
       "      <td>0.839342</td>\n",
       "      <td>0.484676</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.451754</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>3.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wtd_std_Valence</th>\n",
       "      <td>21263.0</td>\n",
       "      <td>0.673987</td>\n",
       "      <td>0.455580</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.306892</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.020436</td>\n",
       "      <td>3.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>critical_temp</th>\n",
       "      <td>21263.0</td>\n",
       "      <td>34.421219</td>\n",
       "      <td>34.254362</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>5.365000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>185.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         count       mean        std       min        25%   \n",
       "number_of_elements     21263.0   4.115224   1.439295  1.000000   3.000000  \\\n",
       "mean_atomic_mass       21263.0  87.557631  29.676497  6.941000  72.458076   \n",
       "wtd_mean_atomic_mass   21263.0  72.988310  33.490406  6.423452  52.143839   \n",
       "gmean_atomic_mass      21263.0  71.290627  31.030272  5.320573  58.041225   \n",
       "wtd_gmean_atomic_mass  21263.0  58.539916  36.651067  1.960849  35.248990   \n",
       "...                        ...        ...        ...       ...        ...   \n",
       "range_Valence          21263.0   2.041010   1.242345  0.000000   1.000000   \n",
       "wtd_range_Valence      21263.0   1.483007   0.978176  0.000000   0.921454   \n",
       "std_Valence            21263.0   0.839342   0.484676  0.000000   0.451754   \n",
       "wtd_std_Valence        21263.0   0.673987   0.455580  0.000000   0.306892   \n",
       "critical_temp          21263.0  34.421219  34.254362  0.000210   5.365000   \n",
       "\n",
       "                             50%         75%       max  \n",
       "number_of_elements      4.000000    5.000000    9.0000  \n",
       "mean_atomic_mass       84.922750  100.404410  208.9804  \n",
       "wtd_mean_atomic_mass   60.696571   86.103540  208.9804  \n",
       "gmean_atomic_mass      66.361592   78.116681  208.9804  \n",
       "wtd_gmean_atomic_mass  39.918385   73.113234  208.9804  \n",
       "...                          ...         ...       ...  \n",
       "range_Valence           2.000000    3.000000    6.0000  \n",
       "wtd_range_Valence       1.063077    1.918400    6.9922  \n",
       "std_Valence             0.800000    1.200000    3.0000  \n",
       "wtd_std_Valence         0.500000    1.020436    3.0000  \n",
       "critical_temp          20.000000   63.000000  185.0000  \n",
       "\n",
       "[82 rows x 8 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_data.describe().T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and the dimensionsof the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21263, 82)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_data.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then extract the predictor variables and the target variables and save the as different objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=temp_data.drop('critical_temp',axis=1)\n",
    "y=np.sqrt(temp_data['critical_temp'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of features is relatively high which points at the possibility of highly correlated variables. This means that these variables provide almost the same information to the model and so it is prudent to remove them in order to make better inference on feature importance and also to manage computational costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wtd_gmean_atomic_mass',\n",
       " 'std_atomic_mass',\n",
       " 'gmean_fie',\n",
       " 'wtd_gmean_fie',\n",
       " 'entropy_fie',\n",
       " 'std_fie',\n",
       " 'wtd_gmean_atomic_radius',\n",
       " 'entropy_atomic_radius',\n",
       " 'wtd_entropy_atomic_radius',\n",
       " 'std_atomic_radius',\n",
       " 'wtd_std_atomic_radius',\n",
       " 'wtd_gmean_Density',\n",
       " 'std_Density',\n",
       " 'std_ElectronAffinity',\n",
       " 'wtd_gmean_FusionHeat',\n",
       " 'std_FusionHeat',\n",
       " 'std_ThermalConductivity',\n",
       " 'wtd_std_ThermalConductivity',\n",
       " 'gmean_Valence',\n",
       " 'wtd_gmean_Valence',\n",
       " 'entropy_Valence',\n",
       " 'wtd_entropy_Valence',\n",
       " 'std_Valence']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrix=X.corr().abs()\n",
    "upper=corr_matrix.where(np.triu(np.ones(corr_matrix.shape),k=1).astype(bool))\n",
    "to_drop=[col for col in X.columns if any(upper[col]>.95)]\n",
    "to_drop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After extracting these columns, we remove them from the predictor set that is used for prediction. There is loss of information from doing this but efficiency gains in model development are a fair tradeoff especially when running on a CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number_of_elements</th>\n",
       "      <th>mean_atomic_mass</th>\n",
       "      <th>wtd_mean_atomic_mass</th>\n",
       "      <th>gmean_atomic_mass</th>\n",
       "      <th>entropy_atomic_mass</th>\n",
       "      <th>wtd_entropy_atomic_mass</th>\n",
       "      <th>range_atomic_mass</th>\n",
       "      <th>wtd_range_atomic_mass</th>\n",
       "      <th>wtd_std_atomic_mass</th>\n",
       "      <th>mean_fie</th>\n",
       "      <th>...</th>\n",
       "      <th>wtd_gmean_ThermalConductivity</th>\n",
       "      <th>entropy_ThermalConductivity</th>\n",
       "      <th>wtd_entropy_ThermalConductivity</th>\n",
       "      <th>range_ThermalConductivity</th>\n",
       "      <th>wtd_range_ThermalConductivity</th>\n",
       "      <th>mean_Valence</th>\n",
       "      <th>wtd_mean_Valence</th>\n",
       "      <th>range_Valence</th>\n",
       "      <th>wtd_range_Valence</th>\n",
       "      <th>wtd_std_Valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>88.944468</td>\n",
       "      <td>57.862692</td>\n",
       "      <td>66.361592</td>\n",
       "      <td>1.181795</td>\n",
       "      <td>1.062396</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>31.794921</td>\n",
       "      <td>53.622535</td>\n",
       "      <td>775.425000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.621979</td>\n",
       "      <td>0.308148</td>\n",
       "      <td>0.262848</td>\n",
       "      <td>399.97342</td>\n",
       "      <td>57.127669</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.257143</td>\n",
       "      <td>1</td>\n",
       "      <td>1.085714</td>\n",
       "      <td>0.437059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>92.729214</td>\n",
       "      <td>58.518416</td>\n",
       "      <td>73.132787</td>\n",
       "      <td>1.449309</td>\n",
       "      <td>1.057755</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>36.161939</td>\n",
       "      <td>53.979870</td>\n",
       "      <td>766.440000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.619735</td>\n",
       "      <td>0.847404</td>\n",
       "      <td>0.567706</td>\n",
       "      <td>429.97342</td>\n",
       "      <td>51.413383</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.257143</td>\n",
       "      <td>2</td>\n",
       "      <td>1.128571</td>\n",
       "      <td>0.468606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>88.944468</td>\n",
       "      <td>57.885242</td>\n",
       "      <td>66.361592</td>\n",
       "      <td>1.181795</td>\n",
       "      <td>0.975980</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>35.741099</td>\n",
       "      <td>53.656268</td>\n",
       "      <td>775.425000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.619095</td>\n",
       "      <td>0.308148</td>\n",
       "      <td>0.250477</td>\n",
       "      <td>399.97342</td>\n",
       "      <td>57.127669</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.271429</td>\n",
       "      <td>1</td>\n",
       "      <td>1.114286</td>\n",
       "      <td>0.444697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>88.944468</td>\n",
       "      <td>57.873967</td>\n",
       "      <td>66.361592</td>\n",
       "      <td>1.181795</td>\n",
       "      <td>1.022291</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>33.768010</td>\n",
       "      <td>53.639405</td>\n",
       "      <td>775.425000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.620535</td>\n",
       "      <td>0.308148</td>\n",
       "      <td>0.257045</td>\n",
       "      <td>399.97342</td>\n",
       "      <td>57.127669</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.264286</td>\n",
       "      <td>1</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>0.440952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>88.944468</td>\n",
       "      <td>57.840143</td>\n",
       "      <td>66.361592</td>\n",
       "      <td>1.181795</td>\n",
       "      <td>1.129224</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>27.848743</td>\n",
       "      <td>53.588771</td>\n",
       "      <td>775.425000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.624878</td>\n",
       "      <td>0.308148</td>\n",
       "      <td>0.272820</td>\n",
       "      <td>399.97342</td>\n",
       "      <td>57.127669</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.242857</td>\n",
       "      <td>1</td>\n",
       "      <td>1.057143</td>\n",
       "      <td>0.428809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21258</th>\n",
       "      <td>4</td>\n",
       "      <td>106.957877</td>\n",
       "      <td>53.095769</td>\n",
       "      <td>82.515384</td>\n",
       "      <td>1.177145</td>\n",
       "      <td>1.254119</td>\n",
       "      <td>146.88130</td>\n",
       "      <td>15.504479</td>\n",
       "      <td>43.202659</td>\n",
       "      <td>661.775000</td>\n",
       "      <td>...</td>\n",
       "      <td>95.001493</td>\n",
       "      <td>1.029002</td>\n",
       "      <td>0.634332</td>\n",
       "      <td>134.00000</td>\n",
       "      <td>83.048889</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3.555556</td>\n",
       "      <td>1</td>\n",
       "      <td>2.168889</td>\n",
       "      <td>0.496904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21259</th>\n",
       "      <td>5</td>\n",
       "      <td>92.266740</td>\n",
       "      <td>49.021367</td>\n",
       "      <td>64.812662</td>\n",
       "      <td>1.323287</td>\n",
       "      <td>1.571630</td>\n",
       "      <td>188.38390</td>\n",
       "      <td>7.353333</td>\n",
       "      <td>50.148287</td>\n",
       "      <td>747.780000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.577047</td>\n",
       "      <td>0.949904</td>\n",
       "      <td>0.745515</td>\n",
       "      <td>399.97342</td>\n",
       "      <td>76.176553</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.047619</td>\n",
       "      <td>1</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.212959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21260</th>\n",
       "      <td>2</td>\n",
       "      <td>99.663190</td>\n",
       "      <td>95.609104</td>\n",
       "      <td>99.433882</td>\n",
       "      <td>0.690847</td>\n",
       "      <td>0.530198</td>\n",
       "      <td>13.51362</td>\n",
       "      <td>53.041104</td>\n",
       "      <td>5.405448</td>\n",
       "      <td>733.550000</td>\n",
       "      <td>...</td>\n",
       "      <td>57.038314</td>\n",
       "      <td>0.683870</td>\n",
       "      <td>0.559446</td>\n",
       "      <td>17.00000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>4.50</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>1</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21261</th>\n",
       "      <td>2</td>\n",
       "      <td>99.663190</td>\n",
       "      <td>97.095602</td>\n",
       "      <td>99.433882</td>\n",
       "      <td>0.690847</td>\n",
       "      <td>0.640883</td>\n",
       "      <td>13.51362</td>\n",
       "      <td>31.115202</td>\n",
       "      <td>6.249958</td>\n",
       "      <td>733.550000</td>\n",
       "      <td>...</td>\n",
       "      <td>58.781651</td>\n",
       "      <td>0.683870</td>\n",
       "      <td>0.659671</td>\n",
       "      <td>17.00000</td>\n",
       "      <td>15.250000</td>\n",
       "      <td>4.50</td>\n",
       "      <td>4.690000</td>\n",
       "      <td>1</td>\n",
       "      <td>2.210000</td>\n",
       "      <td>0.462493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21262</th>\n",
       "      <td>3</td>\n",
       "      <td>87.468333</td>\n",
       "      <td>86.858500</td>\n",
       "      <td>82.555758</td>\n",
       "      <td>1.041270</td>\n",
       "      <td>0.895229</td>\n",
       "      <td>71.75500</td>\n",
       "      <td>43.144000</td>\n",
       "      <td>33.927941</td>\n",
       "      <td>856.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>12.919996</td>\n",
       "      <td>0.194158</td>\n",
       "      <td>0.142553</td>\n",
       "      <td>78.48000</td>\n",
       "      <td>39.448000</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>3</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>1.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21263 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       number_of_elements  mean_atomic_mass  wtd_mean_atomic_mass   \n",
       "0                       4         88.944468             57.862692  \\\n",
       "1                       5         92.729214             58.518416   \n",
       "2                       4         88.944468             57.885242   \n",
       "3                       4         88.944468             57.873967   \n",
       "4                       4         88.944468             57.840143   \n",
       "...                   ...               ...                   ...   \n",
       "21258                   4        106.957877             53.095769   \n",
       "21259                   5         92.266740             49.021367   \n",
       "21260                   2         99.663190             95.609104   \n",
       "21261                   2         99.663190             97.095602   \n",
       "21262                   3         87.468333             86.858500   \n",
       "\n",
       "       gmean_atomic_mass  entropy_atomic_mass  wtd_entropy_atomic_mass   \n",
       "0              66.361592             1.181795                 1.062396  \\\n",
       "1              73.132787             1.449309                 1.057755   \n",
       "2              66.361592             1.181795                 0.975980   \n",
       "3              66.361592             1.181795                 1.022291   \n",
       "4              66.361592             1.181795                 1.129224   \n",
       "...                  ...                  ...                      ...   \n",
       "21258          82.515384             1.177145                 1.254119   \n",
       "21259          64.812662             1.323287                 1.571630   \n",
       "21260          99.433882             0.690847                 0.530198   \n",
       "21261          99.433882             0.690847                 0.640883   \n",
       "21262          82.555758             1.041270                 0.895229   \n",
       "\n",
       "       range_atomic_mass  wtd_range_atomic_mass  wtd_std_atomic_mass   \n",
       "0              122.90607              31.794921            53.622535  \\\n",
       "1              122.90607              36.161939            53.979870   \n",
       "2              122.90607              35.741099            53.656268   \n",
       "3              122.90607              33.768010            53.639405   \n",
       "4              122.90607              27.848743            53.588771   \n",
       "...                  ...                    ...                  ...   \n",
       "21258          146.88130              15.504479            43.202659   \n",
       "21259          188.38390               7.353333            50.148287   \n",
       "21260           13.51362              53.041104             5.405448   \n",
       "21261           13.51362              31.115202             6.249958   \n",
       "21262           71.75500              43.144000            33.927941   \n",
       "\n",
       "         mean_fie  ...  wtd_gmean_ThermalConductivity   \n",
       "0      775.425000  ...                       0.621979  \\\n",
       "1      766.440000  ...                       0.619735   \n",
       "2      775.425000  ...                       0.619095   \n",
       "3      775.425000  ...                       0.620535   \n",
       "4      775.425000  ...                       0.624878   \n",
       "...           ...  ...                            ...   \n",
       "21258  661.775000  ...                      95.001493   \n",
       "21259  747.780000  ...                       1.577047   \n",
       "21260  733.550000  ...                      57.038314   \n",
       "21261  733.550000  ...                      58.781651   \n",
       "21262  856.166667  ...                      12.919996   \n",
       "\n",
       "       entropy_ThermalConductivity  wtd_entropy_ThermalConductivity   \n",
       "0                         0.308148                         0.262848  \\\n",
       "1                         0.847404                         0.567706   \n",
       "2                         0.308148                         0.250477   \n",
       "3                         0.308148                         0.257045   \n",
       "4                         0.308148                         0.272820   \n",
       "...                            ...                              ...   \n",
       "21258                     1.029002                         0.634332   \n",
       "21259                     0.949904                         0.745515   \n",
       "21260                     0.683870                         0.559446   \n",
       "21261                     0.683870                         0.659671   \n",
       "21262                     0.194158                         0.142553   \n",
       "\n",
       "       range_ThermalConductivity  wtd_range_ThermalConductivity  mean_Valence   \n",
       "0                      399.97342                      57.127669          2.25  \\\n",
       "1                      429.97342                      51.413383          2.00   \n",
       "2                      399.97342                      57.127669          2.25   \n",
       "3                      399.97342                      57.127669          2.25   \n",
       "4                      399.97342                      57.127669          2.25   \n",
       "...                          ...                            ...           ...   \n",
       "21258                  134.00000                      83.048889          3.25   \n",
       "21259                  399.97342                      76.176553          2.20   \n",
       "21260                   17.00000                      29.000000          4.50   \n",
       "21261                   17.00000                      15.250000          4.50   \n",
       "21262                   78.48000                      39.448000          5.00   \n",
       "\n",
       "       wtd_mean_Valence  range_Valence  wtd_range_Valence  wtd_std_Valence  \n",
       "0              2.257143              1           1.085714         0.437059  \n",
       "1              2.257143              2           1.128571         0.468606  \n",
       "2              2.271429              1           1.114286         0.444697  \n",
       "3              2.264286              1           1.100000         0.440952  \n",
       "4              2.242857              1           1.057143         0.428809  \n",
       "...                 ...            ...                ...              ...  \n",
       "21258          3.555556              1           2.168889         0.496904  \n",
       "21259          2.047619              1           0.904762         0.212959  \n",
       "21260          4.800000              1           3.200000         0.400000  \n",
       "21261          4.690000              1           2.210000         0.462493  \n",
       "21262          4.500000              3           1.800000         1.500000  \n",
       "\n",
       "[21263 rows x 58 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.drop(to_drop,axis=1,inplace=True)\n",
    "X"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract a small sample of examples of the dataset. This sample will be used to test the API endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number_of_elements</th>\n",
       "      <th>mean_atomic_mass</th>\n",
       "      <th>wtd_mean_atomic_mass</th>\n",
       "      <th>gmean_atomic_mass</th>\n",
       "      <th>entropy_atomic_mass</th>\n",
       "      <th>wtd_entropy_atomic_mass</th>\n",
       "      <th>range_atomic_mass</th>\n",
       "      <th>wtd_range_atomic_mass</th>\n",
       "      <th>wtd_std_atomic_mass</th>\n",
       "      <th>mean_fie</th>\n",
       "      <th>...</th>\n",
       "      <th>wtd_gmean_ThermalConductivity</th>\n",
       "      <th>entropy_ThermalConductivity</th>\n",
       "      <th>wtd_entropy_ThermalConductivity</th>\n",
       "      <th>range_ThermalConductivity</th>\n",
       "      <th>wtd_range_ThermalConductivity</th>\n",
       "      <th>mean_Valence</th>\n",
       "      <th>wtd_mean_Valence</th>\n",
       "      <th>range_Valence</th>\n",
       "      <th>wtd_range_Valence</th>\n",
       "      <th>wtd_std_Valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>88.944468</td>\n",
       "      <td>57.862692</td>\n",
       "      <td>66.361592</td>\n",
       "      <td>1.181795</td>\n",
       "      <td>1.062396</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>31.794921</td>\n",
       "      <td>53.622535</td>\n",
       "      <td>775.425000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.621979</td>\n",
       "      <td>0.308148</td>\n",
       "      <td>0.262848</td>\n",
       "      <td>399.97342</td>\n",
       "      <td>57.127669</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.257143</td>\n",
       "      <td>1</td>\n",
       "      <td>1.085714</td>\n",
       "      <td>0.437059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>92.729214</td>\n",
       "      <td>58.518416</td>\n",
       "      <td>73.132787</td>\n",
       "      <td>1.449309</td>\n",
       "      <td>1.057755</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>36.161939</td>\n",
       "      <td>53.979870</td>\n",
       "      <td>766.440000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.619735</td>\n",
       "      <td>0.847404</td>\n",
       "      <td>0.567706</td>\n",
       "      <td>429.97342</td>\n",
       "      <td>51.413383</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.257143</td>\n",
       "      <td>2</td>\n",
       "      <td>1.128571</td>\n",
       "      <td>0.468606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>88.944468</td>\n",
       "      <td>57.885242</td>\n",
       "      <td>66.361592</td>\n",
       "      <td>1.181795</td>\n",
       "      <td>0.975980</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>35.741099</td>\n",
       "      <td>53.656268</td>\n",
       "      <td>775.425000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.619095</td>\n",
       "      <td>0.308148</td>\n",
       "      <td>0.250477</td>\n",
       "      <td>399.97342</td>\n",
       "      <td>57.127669</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.271429</td>\n",
       "      <td>1</td>\n",
       "      <td>1.114286</td>\n",
       "      <td>0.444697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>88.944468</td>\n",
       "      <td>57.873967</td>\n",
       "      <td>66.361592</td>\n",
       "      <td>1.181795</td>\n",
       "      <td>1.022291</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>33.768010</td>\n",
       "      <td>53.639405</td>\n",
       "      <td>775.425000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.620535</td>\n",
       "      <td>0.308148</td>\n",
       "      <td>0.257045</td>\n",
       "      <td>399.97342</td>\n",
       "      <td>57.127669</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.264286</td>\n",
       "      <td>1</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>0.440952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>88.944468</td>\n",
       "      <td>57.840143</td>\n",
       "      <td>66.361592</td>\n",
       "      <td>1.181795</td>\n",
       "      <td>1.129224</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>27.848743</td>\n",
       "      <td>53.588771</td>\n",
       "      <td>775.425000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.624878</td>\n",
       "      <td>0.308148</td>\n",
       "      <td>0.272820</td>\n",
       "      <td>399.97342</td>\n",
       "      <td>57.127669</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.242857</td>\n",
       "      <td>1</td>\n",
       "      <td>1.057143</td>\n",
       "      <td>0.428809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21258</th>\n",
       "      <td>4</td>\n",
       "      <td>106.957877</td>\n",
       "      <td>53.095769</td>\n",
       "      <td>82.515384</td>\n",
       "      <td>1.177145</td>\n",
       "      <td>1.254119</td>\n",
       "      <td>146.88130</td>\n",
       "      <td>15.504479</td>\n",
       "      <td>43.202659</td>\n",
       "      <td>661.775000</td>\n",
       "      <td>...</td>\n",
       "      <td>95.001493</td>\n",
       "      <td>1.029002</td>\n",
       "      <td>0.634332</td>\n",
       "      <td>134.00000</td>\n",
       "      <td>83.048889</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3.555556</td>\n",
       "      <td>1</td>\n",
       "      <td>2.168889</td>\n",
       "      <td>0.496904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21259</th>\n",
       "      <td>5</td>\n",
       "      <td>92.266740</td>\n",
       "      <td>49.021367</td>\n",
       "      <td>64.812662</td>\n",
       "      <td>1.323287</td>\n",
       "      <td>1.571630</td>\n",
       "      <td>188.38390</td>\n",
       "      <td>7.353333</td>\n",
       "      <td>50.148287</td>\n",
       "      <td>747.780000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.577047</td>\n",
       "      <td>0.949904</td>\n",
       "      <td>0.745515</td>\n",
       "      <td>399.97342</td>\n",
       "      <td>76.176553</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.047619</td>\n",
       "      <td>1</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.212959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21260</th>\n",
       "      <td>2</td>\n",
       "      <td>99.663190</td>\n",
       "      <td>95.609104</td>\n",
       "      <td>99.433882</td>\n",
       "      <td>0.690847</td>\n",
       "      <td>0.530198</td>\n",
       "      <td>13.51362</td>\n",
       "      <td>53.041104</td>\n",
       "      <td>5.405448</td>\n",
       "      <td>733.550000</td>\n",
       "      <td>...</td>\n",
       "      <td>57.038314</td>\n",
       "      <td>0.683870</td>\n",
       "      <td>0.559446</td>\n",
       "      <td>17.00000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>4.50</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>1</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21261</th>\n",
       "      <td>2</td>\n",
       "      <td>99.663190</td>\n",
       "      <td>97.095602</td>\n",
       "      <td>99.433882</td>\n",
       "      <td>0.690847</td>\n",
       "      <td>0.640883</td>\n",
       "      <td>13.51362</td>\n",
       "      <td>31.115202</td>\n",
       "      <td>6.249958</td>\n",
       "      <td>733.550000</td>\n",
       "      <td>...</td>\n",
       "      <td>58.781651</td>\n",
       "      <td>0.683870</td>\n",
       "      <td>0.659671</td>\n",
       "      <td>17.00000</td>\n",
       "      <td>15.250000</td>\n",
       "      <td>4.50</td>\n",
       "      <td>4.690000</td>\n",
       "      <td>1</td>\n",
       "      <td>2.210000</td>\n",
       "      <td>0.462493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21262</th>\n",
       "      <td>3</td>\n",
       "      <td>87.468333</td>\n",
       "      <td>86.858500</td>\n",
       "      <td>82.555758</td>\n",
       "      <td>1.041270</td>\n",
       "      <td>0.895229</td>\n",
       "      <td>71.75500</td>\n",
       "      <td>43.144000</td>\n",
       "      <td>33.927941</td>\n",
       "      <td>856.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>12.919996</td>\n",
       "      <td>0.194158</td>\n",
       "      <td>0.142553</td>\n",
       "      <td>78.48000</td>\n",
       "      <td>39.448000</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>3</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>1.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21258 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       number_of_elements  mean_atomic_mass  wtd_mean_atomic_mass   \n",
       "0                       4         88.944468             57.862692  \\\n",
       "1                       5         92.729214             58.518416   \n",
       "2                       4         88.944468             57.885242   \n",
       "3                       4         88.944468             57.873967   \n",
       "4                       4         88.944468             57.840143   \n",
       "...                   ...               ...                   ...   \n",
       "21258                   4        106.957877             53.095769   \n",
       "21259                   5         92.266740             49.021367   \n",
       "21260                   2         99.663190             95.609104   \n",
       "21261                   2         99.663190             97.095602   \n",
       "21262                   3         87.468333             86.858500   \n",
       "\n",
       "       gmean_atomic_mass  entropy_atomic_mass  wtd_entropy_atomic_mass   \n",
       "0              66.361592             1.181795                 1.062396  \\\n",
       "1              73.132787             1.449309                 1.057755   \n",
       "2              66.361592             1.181795                 0.975980   \n",
       "3              66.361592             1.181795                 1.022291   \n",
       "4              66.361592             1.181795                 1.129224   \n",
       "...                  ...                  ...                      ...   \n",
       "21258          82.515384             1.177145                 1.254119   \n",
       "21259          64.812662             1.323287                 1.571630   \n",
       "21260          99.433882             0.690847                 0.530198   \n",
       "21261          99.433882             0.690847                 0.640883   \n",
       "21262          82.555758             1.041270                 0.895229   \n",
       "\n",
       "       range_atomic_mass  wtd_range_atomic_mass  wtd_std_atomic_mass   \n",
       "0              122.90607              31.794921            53.622535  \\\n",
       "1              122.90607              36.161939            53.979870   \n",
       "2              122.90607              35.741099            53.656268   \n",
       "3              122.90607              33.768010            53.639405   \n",
       "4              122.90607              27.848743            53.588771   \n",
       "...                  ...                    ...                  ...   \n",
       "21258          146.88130              15.504479            43.202659   \n",
       "21259          188.38390               7.353333            50.148287   \n",
       "21260           13.51362              53.041104             5.405448   \n",
       "21261           13.51362              31.115202             6.249958   \n",
       "21262           71.75500              43.144000            33.927941   \n",
       "\n",
       "         mean_fie  ...  wtd_gmean_ThermalConductivity   \n",
       "0      775.425000  ...                       0.621979  \\\n",
       "1      766.440000  ...                       0.619735   \n",
       "2      775.425000  ...                       0.619095   \n",
       "3      775.425000  ...                       0.620535   \n",
       "4      775.425000  ...                       0.624878   \n",
       "...           ...  ...                            ...   \n",
       "21258  661.775000  ...                      95.001493   \n",
       "21259  747.780000  ...                       1.577047   \n",
       "21260  733.550000  ...                      57.038314   \n",
       "21261  733.550000  ...                      58.781651   \n",
       "21262  856.166667  ...                      12.919996   \n",
       "\n",
       "       entropy_ThermalConductivity  wtd_entropy_ThermalConductivity   \n",
       "0                         0.308148                         0.262848  \\\n",
       "1                         0.847404                         0.567706   \n",
       "2                         0.308148                         0.250477   \n",
       "3                         0.308148                         0.257045   \n",
       "4                         0.308148                         0.272820   \n",
       "...                            ...                              ...   \n",
       "21258                     1.029002                         0.634332   \n",
       "21259                     0.949904                         0.745515   \n",
       "21260                     0.683870                         0.559446   \n",
       "21261                     0.683870                         0.659671   \n",
       "21262                     0.194158                         0.142553   \n",
       "\n",
       "       range_ThermalConductivity  wtd_range_ThermalConductivity  mean_Valence   \n",
       "0                      399.97342                      57.127669          2.25  \\\n",
       "1                      429.97342                      51.413383          2.00   \n",
       "2                      399.97342                      57.127669          2.25   \n",
       "3                      399.97342                      57.127669          2.25   \n",
       "4                      399.97342                      57.127669          2.25   \n",
       "...                          ...                            ...           ...   \n",
       "21258                  134.00000                      83.048889          3.25   \n",
       "21259                  399.97342                      76.176553          2.20   \n",
       "21260                   17.00000                      29.000000          4.50   \n",
       "21261                   17.00000                      15.250000          4.50   \n",
       "21262                   78.48000                      39.448000          5.00   \n",
       "\n",
       "       wtd_mean_Valence  range_Valence  wtd_range_Valence  wtd_std_Valence  \n",
       "0              2.257143              1           1.085714         0.437059  \n",
       "1              2.257143              2           1.128571         0.468606  \n",
       "2              2.271429              1           1.114286         0.444697  \n",
       "3              2.264286              1           1.100000         0.440952  \n",
       "4              2.242857              1           1.057143         0.428809  \n",
       "...                 ...            ...                ...              ...  \n",
       "21258          3.555556              1           2.168889         0.496904  \n",
       "21259          2.047619              1           0.904762         0.212959  \n",
       "21260          4.800000              1           3.200000         0.400000  \n",
       "21261          4.690000              1           2.210000         0.462493  \n",
       "21262          4.500000              3           1.800000         1.500000  \n",
       "\n",
       "[21258 rows x 58 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data_index=X.sample(5).index\n",
    "X.iloc[sample_data_index].to_csv('../temp_fastapi/data/sample_data.csv',\n",
    "                   index_label=False)\n",
    "X.drop(sample_data_index,axis=0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also extract the columns names of the data which will be used in defining the structure of the prediction models in the API. This is important because manually typing in each column name is tedious and at most times data will have upwards of 100 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "cols_dict=X.dtypes.to_dict()\n",
    "\n",
    "new_cols={}\n",
    "for col, type in cols_dict.items():\n",
    "    if type == 'int64':\n",
    "        new_cols[col] = 'int'\n",
    "    else:\n",
    "        new_cols[col] = 'float'\n",
    "\n",
    "with open('../temp_fastapi/data/cols.json','w') as f:\n",
    "    json.dump(new_cols,f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target(dependent) variable appears to be multimodal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.45794468e-02, 5.75285691e-02, 1.11941873e-01, 1.64589859e-01,\n",
       "        1.71235759e-01, 1.46832846e-01, 1.29283517e-01, 1.03426814e-01,\n",
       "        9.98961795e-02, 1.01142286e-01, 7.61163197e-02, 8.96158035e-02,\n",
       "        9.33541220e-02, 9.14849627e-02, 5.14018803e-02, 5.07788272e-02,\n",
       "        5.35825661e-02, 6.15784142e-02, 7.91277430e-02, 1.17237824e-01,\n",
       "        1.39460051e-01, 1.00311548e-01, 3.11526547e-02, 2.62720722e-02,\n",
       "        1.50571165e-02, 1.06957448e-02, 2.07684365e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.03842182e-04]),\n",
       " array([ 0.01449138,  0.46739068,  0.92028999,  1.37318929,  1.82608859,\n",
       "         2.2789879 ,  2.7318872 ,  3.18478651,  3.63768581,  4.09058512,\n",
       "         4.54348442,  4.99638373,  5.44928303,  5.90218233,  6.35508164,\n",
       "         6.80798094,  7.26088025,  7.71377955,  8.16667886,  8.61957816,\n",
       "         9.07247746,  9.52537677,  9.97827607, 10.43117538, 10.88407468,\n",
       "        11.33697399, 11.78987329, 12.2427726 , 12.6956719 , 13.1485712 ,\n",
       "        13.60147051]),\n",
       " <BarContainer object of 30 artists>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y,bins=30,density=True,)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin model development, we split the data int a training set and testing set which will be used for algorithm training and performance evaluation respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(\n",
    "    X,y,test_size=.3,random_state=36)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, we build a couple of models on the training set and choose the best model for prediction based on performance. However, this is a demonstrative project and so all models will be considered for inference."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Support Vector Regression**\n",
    "The support vector machine is an algorithm that initially developed for classification tasks but has been extended to regression tasks as well. It is relatively simple and has lesser computational costs. This model however requires that the features are standardized. This improves performance as the features are on the same scale."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is evaluated on 5 cross-validated folds with the final performance of the model averaged across the folds. With 5 folds, the model is trained using 4 folds and evaluated on the remaining fold. This is repeated until each fold is used for evaluation. \n",
    "\n",
    "To further improve performance and reduce computational costs, the PCA algorithm is used to filter a smaller subset of the features combination which provide a high threshold of information gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "pipe_svr=make_pipeline(StandardScaler(),PCA(n_components=40),LinearSVR())\n",
    "scores=cross_val_score(estimator=pipe_svr,X=X_train,y=y_train,cv=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then check the performance of the model on the folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.732949260274879"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is quite low but it can be used baseline model aginsta which we evaluate more complex models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this level of performance, we then make a final fit of the model on the training data and evaluate on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                (&#x27;pca&#x27;, PCA(n_components=40)), (&#x27;linearsvr&#x27;, LinearSVR())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                (&#x27;pca&#x27;, PCA(n_components=40)), (&#x27;linearsvr&#x27;, LinearSVR())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA(n_components=40)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVR</label><div class=\"sk-toggleable__content\"><pre>LinearSVR()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('pca', PCA(n_components=40)), ('linearsvr', LinearSVR())])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_svr.fit(X_train,y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that SVR models performance poorly on unseen data. This hints at the risk of overfittig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7219547557785726"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_svr.score(X_test,y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save/serialize the model object for use in another context as a .joblib object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../temp_fastapi/saved_models/temp_svr_model.joblib']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(pipe_svr,'../temp_fastapi/saved_models/temp_svr_model.joblib')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest Regression**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a more robust model, we use a more complex model. The Random Forest algorithm is an *Ensemble* type of model which involves fitting many decision trees on the data and aggregating the results of each tree in order to arrive at a final model. The rationale for this is that indivindual trees will have high variance which makes it more likely to overfit. To reduce overfitting, many trees are built and their result aggregated. The trees are also pruned by a applying a penalty to more complicated trees. This is called *pruning*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, tuning these parameters was done(code not shown) and the following parameters,i.e, number of trees, maximum tree depth etc were selected from the best combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8955843834292286"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf=RandomForestRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth=10,\n",
    "    criterion='squared_error',\n",
    "    n_jobs=-1,\n",
    "    max_features='sqrt',\n",
    "    bootstrap=True,\n",
    "    max_samples=.8,\n",
    "    warm_start=True)\n",
    "\n",
    "scores=cross_val_score(rf,X_train,y_train)\n",
    "scores.mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the final fit, we see that the results of the Random Forest algorithm have greater improvement in performance over the Support Vector Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_depth=10, max_features=&#x27;sqrt&#x27;, max_samples=0.8,\n",
       "                      n_estimators=500, n_jobs=-1, warm_start=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=10, max_features=&#x27;sqrt&#x27;, max_samples=0.8,\n",
       "                      n_estimators=500, n_jobs=-1, warm_start=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(max_depth=10, max_features='sqrt', max_samples=0.8,\n",
       "                      n_estimators=500, n_jobs=-1, warm_start=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(X_train,y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting has nearly been reduced to zero in this case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8920928451125301"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(X_test,y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then save the model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../temp_fastapi/saved_models/temp_rf_model.joblib']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(rf,'../temp_fastapi/saved_models/temp_rf_model.joblib')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LightGbm Regression**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another model that has been used to achieve higher performance of machine learning algorithms is the gradient boosting regression. This algorithims builds a series of weak learners which are sequentially trained on the errors of the previous learner. It is a powerful algorithm that is mostly used in large scale machine learning tasks. There are variants of this algorithm; the eXtreme Gradient Boosting (*XGBoost*) and the *LightGBM*. The models are quite similar but LightGBm has better performance and lesser computational costs as well.\n",
    "\n",
    "As was the case in the Random Forest Regression, hyperparameter tuning is not shown here but the best combination of parameters is selected here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "lgbm=lgb.LGBMRegressor(\n",
    "    n_estimators=1000,\n",
    "    colsample_bytree=.8,\n",
    "    learning_rate=.1,\n",
    "    max_depth=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9276982583149322"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "lgbm.fit(X_train,y_train)\n",
    "lgbm.score(X_test,y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This models provides significant improvement over the Random Forest Regression. We can we this as the best models as the scope of this project is a demonstration of deployment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../temp_fastapi/saved_models/temp_lightgbm_model.joblib']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(lgbm,'../temp_fastapi/saved_models/temp_lightgbm_model.joblib')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out saved model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMRegressor(colsample_bytree=0.8, n_estimators=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMRegressor</label><div class=\"sk-toggleable__content\"><pre>LGBMRegressor(colsample_bytree=0.8, n_estimators=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMRegressor(colsample_bytree=0.8, n_estimators=1000)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model=joblib.load('../temp_fastapi/saved_models/temp_lightgbm_model.joblib')\n",
    "test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number_of_elements</th>\n",
       "      <th>mean_atomic_mass</th>\n",
       "      <th>wtd_mean_atomic_mass</th>\n",
       "      <th>gmean_atomic_mass</th>\n",
       "      <th>entropy_atomic_mass</th>\n",
       "      <th>wtd_entropy_atomic_mass</th>\n",
       "      <th>range_atomic_mass</th>\n",
       "      <th>wtd_range_atomic_mass</th>\n",
       "      <th>wtd_std_atomic_mass</th>\n",
       "      <th>mean_fie</th>\n",
       "      <th>...</th>\n",
       "      <th>wtd_gmean_ThermalConductivity</th>\n",
       "      <th>entropy_ThermalConductivity</th>\n",
       "      <th>wtd_entropy_ThermalConductivity</th>\n",
       "      <th>range_ThermalConductivity</th>\n",
       "      <th>wtd_range_ThermalConductivity</th>\n",
       "      <th>mean_Valence</th>\n",
       "      <th>wtd_mean_Valence</th>\n",
       "      <th>range_Valence</th>\n",
       "      <th>wtd_range_Valence</th>\n",
       "      <th>wtd_std_Valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8340</th>\n",
       "      <td>5</td>\n",
       "      <td>89.33718</td>\n",
       "      <td>52.143003</td>\n",
       "      <td>70.560647</td>\n",
       "      <td>1.444537</td>\n",
       "      <td>1.389451</td>\n",
       "      <td>124.90825</td>\n",
       "      <td>19.010603</td>\n",
       "      <td>45.345269</td>\n",
       "      <td>740.52</td>\n",
       "      <td>...</td>\n",
       "      <td>1.102786</td>\n",
       "      <td>0.45781</td>\n",
       "      <td>0.206207</td>\n",
       "      <td>399.97342</td>\n",
       "      <td>92.542605</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.092557</td>\n",
       "      <td>2</td>\n",
       "      <td>1.012727</td>\n",
       "      <td>0.338884</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      number_of_elements  mean_atomic_mass  wtd_mean_atomic_mass   \n",
       "8340                   5          89.33718             52.143003  \\\n",
       "\n",
       "      gmean_atomic_mass  entropy_atomic_mass  wtd_entropy_atomic_mass   \n",
       "8340          70.560647             1.444537                 1.389451  \\\n",
       "\n",
       "      range_atomic_mass  wtd_range_atomic_mass  wtd_std_atomic_mass  mean_fie   \n",
       "8340          124.90825              19.010603            45.345269    740.52  \\\n",
       "\n",
       "      ...  wtd_gmean_ThermalConductivity  entropy_ThermalConductivity   \n",
       "8340  ...                       1.102786                      0.45781  \\\n",
       "\n",
       "      wtd_entropy_ThermalConductivity  range_ThermalConductivity   \n",
       "8340                         0.206207                  399.97342  \\\n",
       "\n",
       "      wtd_range_ThermalConductivity  mean_Valence  wtd_mean_Valence   \n",
       "8340                      92.542605           2.6          2.092557  \\\n",
       "\n",
       "      range_Valence  wtd_range_Valence  wtd_std_Valence  \n",
       "8340              2           1.012727         0.338884  \n",
       "\n",
       "[1 rows x 58 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example=X_test.iloc[[np.random.randint(low=1,high=X_test.shape[0]),]]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.412264156534869"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(test_model.predict(example))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
